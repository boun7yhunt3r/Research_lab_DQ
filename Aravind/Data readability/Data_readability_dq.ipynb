{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import enchant\n",
    "import language_tool_python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import stanfordnlp\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'Reviews.csv'\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the calculation functions for each indicator\n",
    "def calculate_scores(text):\n",
    "    # Implement your scoring logic for each indicator\n",
    "    # Return a dictionary with the scores\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    # Overall LI (Tika): Length of the text\n",
    "    scores['Overall LI (Tika)'] = len(text)\n",
    "\n",
    "    # POS (CRF): Part-of-speech tagging\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    scores['POS (CRF)'] = len(pos_tags) / len(tokens)\n",
    "\n",
    "    # Abbreviations (1): Percentage of abbreviations\n",
    "    scores['Abbreviations (1)'] = calculate_percentage_of_abbreviations(text)\n",
    "\n",
    "    # Spelling (2): Number of spelling mistakes\n",
    "    scores['Spelling (2)'] = calculate_spelling_mistakes(text)\n",
    "\n",
    "    # Lexical Diversity (3): Lexical diversity score\n",
    "    scores['Lexical Diversity (3)'] = calculate_lexical_diversity(text)\n",
    "\n",
    "    # Uppercased (4): Percentage of uppercased words\n",
    "    scores['Uppercased (4)'] = calculate_percentage_of_uppercased(text)\n",
    "\n",
    "    # Ungrammatical (5): Percentage of ungrammatical sentences\n",
    "    scores['Ungrammatical (5)'] = calculate_percentage_of_ungrammatical_sentences(text)\n",
    "\n",
    "    # Avg. Sentence Length (6): Average sentence length\n",
    "    scores['Avg. Sentence Length (6)'] = calculate_average_sentence_length(text)\n",
    "\n",
    "    # Fit of training data (7): Measure of text similarity with training data\n",
    "    scores['Fit of training data (7)'] = calculate_fit_of_training_data(text)\n",
    "\n",
    "    # Confidence (8): Confidence score of text analysis modules\n",
    "    scores['Confidence (8)'] = calculate_confidence(text)\n",
    "\n",
    "    # Unknown words (9): Percentage of unknown words\n",
    "    scores['Unknown words (9)'] = calculate_percentage_of_unknown_words(text)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def calculate_percentage_of_abbreviations(text):\n",
    "    st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')  # need to download and load the model\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_words = st.tag(tokens)\n",
    "    total_words = len(tagged_words)\n",
    "    abbreviation_count = sum(1 for word, tag in tagged_words if tag == 'ORGANIZATION')\n",
    "    percentage = abbreviation_count / total_words * 100\n",
    "    return percentage\n",
    "\n",
    "def calculate_spelling_mistakes(text):\n",
    "    dictionary = enchant.Dict(\"en_US\")  # English dictionary\n",
    "    words = text.split()\n",
    "    spelling_mistake_count = sum(1 for word in words if not dictionary.check(word))\n",
    "    return spelling_mistake_count\n",
    "\n",
    "def calculate_lexical_diversity(text):\n",
    "    return (len(set(text)) / len(text))\n",
    "\n",
    "def calculate_percentage_of_uppercased(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_words = len(tokens)\n",
    "    uppercased_words = [word for word in tokens if word.isupper()]\n",
    "    percentage = len(uppercased_words) / total_words * 100\n",
    "    return percentage\n",
    "\n",
    "def calculate_percentage_of_ungrammatical_sentences(text):\n",
    "    tool = language_tool_python.LanguageTool('en-US')  # LanguageTool instance for English language\n",
    "    sentences = text.split('. ')  # Split text into sentences (assuming sentences end with a period and space)\n",
    "    total_sentences = len(sentences)\n",
    "    ungrammatical_sentence_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        matches = tool.check(sentence)\n",
    "        if len(matches) > 0:\n",
    "            ungrammatical_sentence_count += 1\n",
    "\n",
    "    percentage = ungrammatical_sentence_count / total_sentences * 100\n",
    "    return percentage\n",
    "\n",
    "def calculate_average_sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        total_words += len(words)\n",
    "\n",
    "    average_length = total_words / total_sentences\n",
    "    return average_length\n",
    "\n",
    "def calculate_fit_of_training_data(text):\n",
    "    # Define the default training data\n",
    "    default_training_data = [\n",
    "        \"This is the default training data.\",\n",
    "        \"You can add more sentences to improve the training.\",\n",
    "        \"The fit of training data measures the similarity between text and training data.\",\n",
    "    ]\n",
    "\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the default training data\n",
    "    default_training_vectors = vectorizer.fit_transform(default_training_data)\n",
    "\n",
    "    # Transform the given text\n",
    "    text_vector = vectorizer.transform([text])\n",
    "\n",
    "    # Calculate the cosine similarity between the text and training data\n",
    "    similarity_scores = cosine_similarity(text_vector, default_training_vectors)\n",
    "\n",
    "    # Take the maximum similarity score as the fit of training data\n",
    "    fit_score = similarity_scores.max()\n",
    "\n",
    "    return fit_score\n",
    "\n",
    "def calculate_confidence(text):\n",
    "    # Load the English POS tagger model\n",
    "    nlp = stanfordnlp.Pipeline(processors='pos', lang='en')\n",
    "\n",
    "    # Process the text to obtain POS tags\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "    # Extract confidence scores (if available)\n",
    "    confidence_scores = [word.upos_prob if hasattr(word, 'upos_prob') else 1.0 for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "    # Calculate the average confidence score\n",
    "    confidence_score = sum(confidence_scores) / len(confidence_scores)\n",
    "\n",
    "    return confidence_score\n",
    "\n",
    "def calculate_percentage_of_unknown_words(text):\n",
    "    # Tokenize the text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Load a pre-trained English language model\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('words')\n",
    "\n",
    "    # Get the set of known English words\n",
    "    known_words = set(nltk.corpus.words.words())\n",
    "\n",
    "    # Count the number of unknown words\n",
    "    unknown_words = [word for word in words if word.lower() not in known_words]\n",
    "\n",
    "    # Calculate the percentage of unknown words\n",
    "    percentage = (len(unknown_words) / len(words)) * 100\n",
    "\n",
    "    return percentage\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    text_scores = calculate_scores(row['Text'])\n",
    "    summary_scores = calculate_scores(row['Summary'])\n",
    "    \n",
    "    # Print the scores for each attribute\n",
    "    print(f\"Text Scores: {text_scores}\")\n",
    "    print(f\"Summary Scores: {summary_scores}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
